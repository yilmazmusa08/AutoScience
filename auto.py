import os
import json
import uvicorn
import warnings
import numpy as np
import pandas as pd
from typing import List
from fastapi import FastAPI, File, UploadFile, APIRouter
from typing import List, Dict
from pydantic import BaseModel
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings('ignore')
pd.set_option('display.max_rows', 50)
pd.set_option('display.max_columns', None)
warnings.filterwarnings('ignore', category=ConvergenceWarning)
from fastapi import FastAPI, Form, Request, Body
from type_pred import *

import sys

# models klasÃ¶rÃ¼nÃ¼n tam yolu
models_path = '/home/firengiz/Belgeler/proje/automl/models'

# models_path'i sys.path listesine ekle
sys.path.append(models_path)
from init import *
from sklearn.exceptions import ConvergenceWarning
from sklearn.decomposition import PCA
from fastapi.templating import Jinja2Templates
from starlette.staticfiles import StaticFiles
from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse


class OutputData(BaseModel):
    data: List[dict]


app = FastAPI()

templates = Jinja2Templates(directory="template")

# Define a route for serving static files
app.mount("/dist", StaticFiles(directory="dist"), name="dist")
app.mount("/plugins", StaticFiles(directory="plugins"), name="plugins")
app.mount("/static", StaticFiles(directory="static"), name="static")

# HTML page setup
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    # Render the HTML template
    return templates.TemplateResponse("index.html", {"request": request})


import traceback


@app.post('/', response_class=HTMLResponse)
async def run_model_api(request: Request, file: UploadFile = File(...), target: str = Form(None), date: str = Form(None)):
    df = pd.read_csv(file.file)
    sonuc2 = create_model(df, date=date, target=target)
    sonuc2 = set_to_list(sonuc2)

    output_file2 = os.path.join(os.getcwd(), "output2.json")
    with open(output_file2, "w") as f:
        json.dump(sonuc2, f)

    # '/process' endpointine yÃ¶nlendirme yap
    return RedirectResponse(url="/process", status_code=302)



@app.get("/process", response_class=HTMLResponse)
async def run_process_model(request: Request):
    try:
        with open("output1.json", "r") as f1, open("output2.json", "r") as f2:
            output_dict2 = json.load(f1)

        # Render the HTML template
        return templates.TemplateResponse("index.html", {"request": request, "result": json.dumps(output_dict2)})
    except:
        return "Dataset kabul edilmedi, bir hata oluÅŸtu."


@app.post("/", response_class=HTMLResponse)
async def run_analysis_api(request: Request, file: UploadFile = File(...), target: str = Form(None), warning: bool = Form(True), return_stats: bool = Form(False), comp_ratio: float = Form(1.0)):
    try:
        df = pd.read_csv(file.file)

        sonuc = analysis(df, target=target, warning=warning, return_stats=return_stats) if target else analysis(df, target=None, warning=warning, return_stats=return_stats)

        pca_dict = {}
        for col in sonuc['Role']:
            null_counts = df.isnull().sum()
            empty_cols = null_counts[null_counts >= len(df) * 0.6].index
            df.drop(empty_cols, axis=1, inplace=True)

            for col in df.columns:
                if df[col].dtype == 'object' and df[col].nunique() < 20:
                    df[col].fillna(df[col].mode()[0], inplace=True)
                    le = LabelEncoder()
                    df[col] = le.fit_transform(df[col])
                elif df[col].dtype != 'object':
                    df[col].fillna(df[col].mean(), inplace=True)

            df[col] = df[col].fillna(df[col].mean())
            result_dict = calculate_pca(df.select_dtypes(include=['float', 'int']), comp_ratio=comp_ratio)
            pca_dict = {
                'Cumulative Explained Variance Ratio': result_dict['Cumulative Explained Variance Ratio'],
                'Principal Component': result_dict['Principal Component']
            }

        sonuc['PCA'] = pca_dict
        sonuc = set_to_list(sonuc)
        output_file = os.path.join(os.getcwd(), "output.json")

        with open(output_file, "w") as f:
            json.dump(sonuc, f)

        # Redirect to the '/process' endpoint
        return RedirectResponse(url="/process", status_code=302)
    except Exception as e:
        traceback.print_exc()  # Hata mesajÄ±nÄ± terminale yazdÄ±r
        return "Dataset kabul edilmedi, bir hata oluÅŸtu."


@app.get("/process", response_class=HTMLResponse)
async def run_process(request: Request):
    try:
        with open("output.json", "r") as f:
            output_dict = json.load(f)

        # Render the HTML template
        return templates.TemplateResponse("index.html", {"request": request, "result": json.dumps(output_dict)})
    except:
        return "Dataset kabul edilmedi, bir hata oluÅŸtu."


"""@app.post("/model")
async def process_csv(file: UploadFile = File(...), target: str = None):
    # YÃ¼klenen CSV dosyasÄ±nÄ± oku
    df = pd.read_csv(file.file)

    # Verileri Ã¶n iÅŸleme fonksiyonunu Ã§aÄŸÄ±r
    df = preprocess(df)

    # Model oluÅŸturma fonksiyonunu Ã§aÄŸÄ±r
    result = create_model(df, target=target)

    return result"""


@app.get("/")
def Description():
    """
    # ProgramÄ±n amacÄ±, veri biliminde veri Ã¶n iÅŸleme iÅŸlemlerinin otomatikleÅŸtirilmesine yardÄ±mcÄ± olmaktÄ±r.ğŸ’» ğŸ’¡
    Bu program veri Ã¶n iÅŸleme adÄ±mlarÄ±nÄ±n bir kÄ±smÄ±nÄ± otomatik hale getirerek veri biliminde Ã§alÄ±ÅŸanlara zaman kazandÄ±rmayÄ± hedeflemektedir.

    Veri Ã¶n iÅŸleme iÅŸlemlerinin bir kÄ±smÄ±nÄ± otomatik hale getirerek veri bilimi alanÄ±nda 
    Ã§alÄ±ÅŸanlara zaman kazandÄ±rmak, daha hÄ±zlÄ± ve doÄŸru sonuÃ§lar elde etmelerine yardÄ±mcÄ± olmaktÄ±r. Bu otomasyon, veri bilimi uzmanlarÄ±na daha fazla
    zaman kazandÄ±rarak veri analizine daha fazla odaklanmalarÄ±nÄ± saÄŸlayabilir.

    Bu fonksiyon, bir veri setinin analiz edilmesi iÃ§in tasarlanmÄ±ÅŸtÄ±r. Fonksiyon, veri setindeki yÃ¼ksek korelasyona sahip sÃ¼tunlarÄ± ve 
    hedef deÄŸiÅŸkenle yÃ¼ksek korelasyona sahip sÃ¼tunlarÄ± belirleyebilir. Fonksiyon ayrÄ±ca her sÃ¼tunun hangi daÄŸÄ±lÄ±ma sahip olduÄŸunu tahmin edebiliyor.
    Bu Ã¶zelliklerin yanÄ± sÄ±ra, fonksiyon, veri seti istatistiklerini iÃ§eren bir sÃ¶zlÃ¼k dÃ¶ndÃ¼rÃ¼yor.

    # Fonksiyonun parametreleri:
    #
            df: analiz edilecek veri seti
            target: hedef deÄŸiÅŸkeni belirleyen bir boolean (varsayÄ±lan olarak None)
            return_stats: veri seti istatistiklerini iÃ§eren bir sÃ¶zlÃ¼k dÃ¶ndÃ¼rmek iÃ§in bir boolean (varsayÄ±lan olarak False)

    Kod, sÃ¼tunlardaki benzersiz deÄŸerlerin frekans daÄŸÄ±lÄ±mÄ±nÄ±n normalleÅŸtiriilmiÅŸ deÄŸerlerinin ortalamasÄ±, standart sapmasÄ±, ortalama kelime sayÄ±sÄ±,
    kelime sayÄ±larÄ±nÄ±n standart sapmasÄ±, karakter sayÄ±sÄ±nÄ±n ortalamasÄ±, karakter sayÄ±larÄ±nÄ±n standart sapmasÄ±, benzersiz deÄŸerlerin sayÄ±sÄ±, en sÄ±k gÃ¶rÃ¼len deÄŸer, 
    en sÄ±k gÃ¶rÃ¼len deÄŸerin oranÄ±, benzersiz deÄŸerlerin oranÄ± ve deÄŸerlerin benzersiz olup olmadÄ±ÄŸÄ± gibi Ã¶zelliklerini hesaplar.

    # Kod nasÄ±l Ã§alÄ±ÅŸÄ±r ?...
    # 
            1) df,target = ...( target yazÄ±lÄ±rsa:target'i tÄ±rnak iÃ§inde yazmaya gerek yok. ), return_stats=True --> Role, Warning,high_corr_target
               feature_importance, distributions, prblem_type
            2) df,target = None, return_stats=True --> Role, Warning, high_corr_target, prblem_type
            Not: EÄŸer target=None ise o zaman  high_corr_target ve feature_importance sonuÃ§larÄ±nÄ± vermez ( targete'e baÄŸlÄ± Ã§alÄ±ÅŸÄ±yorlar )
                 target = None,Send empty value tik'e tÄ±klamanÄ±z yerterli olucaktÄ±r 
    #
            3) df,target = None, return_stats=False --> Role, Warning, distributions, high_corr_target, prblem_type
            4) df,target = None, return_stats=False --> Role, Warning, distributions, high_corr_target, feature_importance, prblem_type
    """
    return Description

uvicorn.run(app, host="127.0.0.1", port=5050)

# kodu durdurmakÄ±cÄ±n bu kodu yaz: sudo lsof -i :8000    sudo kill 12345(buraya  durdurmak Ä±stedÄ±gÄ±n  PID   numarasÄ±nÄ± yaz)
